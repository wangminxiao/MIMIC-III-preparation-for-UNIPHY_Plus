{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea119fda",
   "metadata": {},
   "source": [
    "# find the high quality subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading lab + vital tables ===\n",
      "Total SUBJECT/HADM with lab+vital data: 14329\n",
      "\n",
      "=== Scanning waveform files ===\n",
      "Total files in waveform dir: 4282\n",
      "Parsed waveform entries: 4282\n",
      "\n",
      "Merged lab+vital+wave entries: 4281\n",
      "\n",
      "=== Computing STRICT Lab–Waveform and Vital–Waveform Overlap ===\n",
      "\n",
      "=== Selecting HIGH-QUALITY data (labs + vitals + waveform) ===\n",
      "\n",
      "High-quality entries: 2694\n",
      "\n",
      "Saved 2694 entries to mimic_high_quality_info_list.json\n",
      "Example entry: ['107_174162_0_PLETH40_II120_II500_2673.npz', 174162, 0, 2672, 2672, 0]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "=================\n",
    "# CONFIGURATION\n",
    "=================\n",
    "HIST_LAB_PATH = \"./mimic_wav_lab_history.csv\"\n",
    "CURR_LAB_PATH = \"./mimic_wav_lab_overlap.csv\"\n",
    "\n",
    "HIST_VITAL_PATH = \"./mimic_wav_vital_history.csv\"\n",
    "CURR_VITAL_PATH = \"./mimic_wav_vital_overlap.csv\"\n",
    "\n",
    "WAVEFORM_DIR = \"/opt/localdata100tb/UNIPHY_Plus/dataset/EST/MIMIC3_SPO2_I_40hz_v3\"\n",
    "\n",
    "OUTPUT_LIST = \"mimic_high_quality_info_list.json\"\n",
    "\n",
    "lab_cols = [\n",
    "    \"Potassium\",\"Calcium\",\"Sodium\",\"Glucose\",\n",
    "    \"Lactate\",\"Creatinine\"\n",
    "]\n",
    "\n",
    "vital_cols = [\n",
    "    \"GM\",\n",
    "    \"ABPs\",\"ABPd\",\"ABPm\",\n",
    "    \"NBPs\",\"NBPd\",\"NBPm\"\n",
    "]\n",
    "\n",
    "=================\n",
    "# STAGE A — LOAD LAB + VITAL TABLES\n",
    "=================\n",
    "\n",
    "print(\"\\n=== Loading lab + vital tables ===\")\n",
    "df_hist_lab = pd.read_csv(HIST_LAB_PATH, parse_dates=[\"CHARTTIME\"])\n",
    "df_curr_lab = pd.read_csv(CURR_LAB_PATH, parse_dates=[\"CHARTTIME\"])\n",
    "\n",
    "df_hist_vit = pd.read_csv(HIST_VITAL_PATH, parse_dates=[\"CHARTTIME\"])\n",
    "df_curr_vit = pd.read_csv(CURR_VITAL_PATH, parse_dates=[\"CHARTTIME\"])\n",
    "\n",
    "\n",
    "# ---------- LAB summary ----------\n",
    "df_curr_lab_summary = (\n",
    "    df_curr_lab.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[lab_cols]\n",
    "               .count()\n",
    "               .reset_index()\n",
    ")\n",
    "df_curr_lab_summary = df_curr_lab_summary.rename(columns={c:f\"{c}_curr\" for c in lab_cols})\n",
    "\n",
    "df_hist_lab_summary = (\n",
    "    df_hist_lab.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[lab_cols]\n",
    "               .count()\n",
    "               .reset_index()\n",
    ")\n",
    "df_hist_lab_summary = df_hist_lab_summary.rename(columns={c:f\"{c}_hist\" for c in lab_cols})\n",
    "\n",
    "lab_all = df_curr_lab_summary.merge(df_hist_lab_summary,\n",
    "                                    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "                                    how=\"outer\").fillna(0)\n",
    "\n",
    "lab_all[\"curr_lab_types_present\"] = (lab_all[[f\"{c}_curr\" for c in lab_cols]] > 0).sum(axis=1)\n",
    "lab_all[\"curr_lab_types_dense\"]   = (lab_all[[f\"{c}_curr\" for c in lab_cols]] >= 2).sum(axis=1)\n",
    "\n",
    "\n",
    "# ---------- VITAL summary ----------\n",
    "df_curr_vit_summary = (\n",
    "    df_curr_vit.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[vital_cols]\n",
    "               .count()\n",
    "               .reset_index()\n",
    ")\n",
    "df_curr_vit_summary = df_curr_vit_summary.rename(columns={c:f\"{c}_curr_v\" for c in vital_cols})\n",
    "\n",
    "df_hist_vit_summary = (\n",
    "    df_hist_vit.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[vital_cols]\n",
    "               .count()\n",
    "               .reset_index()\n",
    ")\n",
    "df_hist_vit_summary = df_hist_vit_summary.rename(columns={c:f\"{c}_hist_v\" for c in vital_cols})\n",
    "\n",
    "vital_all = df_curr_vit_summary.merge(df_hist_vit_summary,\n",
    "                                      on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "                                      how=\"outer\").fillna(0)\n",
    "\n",
    "vital_all[\"curr_vital_types_present\"] = (vital_all[[f\"{c}_curr_v\" for c in vital_cols]] > 0).sum(axis=1)\n",
    "vital_all[\"curr_vital_types_dense\"]   = (vital_all[[f\"{c}_curr_v\" for c in vital_cols]] >= 3).sum(axis=1)\n",
    "\n",
    "print(\"Total SUBJECT/HADM with lab+vital data:\", len(lab_all))\n",
    "\n",
    "\n",
    "=================\n",
    "# STAGE B — LOAD WAVEFORM METADATA FROM NPZ FILES\n",
    "=================\n",
    "\n",
    "print(\"\\n=== Scanning waveform files ===\")\n",
    "\n",
    "wave_recs = []\n",
    "files = [f for f in os.listdir(WAVEFORM_DIR) if f.endswith(\".npz\")]\n",
    "print(\"Total files in waveform dir:\", len(files))\n",
    "\n",
    "for fname in files:\n",
    "    base = fname[:-4]\n",
    "    parts = base.split(\"_\")\n",
    "    if len(parts) < 7:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        subj_id = int(parts[0])\n",
    "        hadm_id = int(parts[1])\n",
    "        clip_str = parts[2]\n",
    "        nseg = int(parts[-1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        clip_raw_index = [int(x) for x in clip_str.split(\"-\")]\n",
    "    except:\n",
    "        clip_raw_index = []\n",
    "\n",
    "    arr = np.load(os.path.join(WAVEFORM_DIR, fname), allow_pickle=True)\n",
    "    if \"time\" not in arr:\n",
    "        continue\n",
    "\n",
    "    time_ms = arr[\"time\"]\n",
    "    if len(time_ms) == 0:\n",
    "        continue\n",
    "\n",
    "    wave_start_dt = pd.to_datetime(time_ms[0], unit=\"ms\", errors=\"coerce\")\n",
    "    wave_end_dt   = pd.to_datetime(time_ms[-1] + 30000, unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    wave_recs.append((\n",
    "        subj_id,\n",
    "        hadm_id,\n",
    "        fname,\n",
    "        nseg,\n",
    "        wave_start_dt,\n",
    "        wave_end_dt,\n",
    "        clip_raw_index\n",
    "    ))\n",
    "\n",
    "df_wave = pd.DataFrame(\n",
    "    wave_recs,\n",
    "    columns=[\n",
    "        \"SUBJECT_ID\",\"HADM_ID\",\"file\",\"nseg\",\n",
    "        \"wave_start_dt\",\"wave_end_dt\",\"clip_raw_index\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Parsed waveform entries:\", df_wave.shape[0])\n",
    "\n",
    "\n",
    "=================\n",
    "# STAGE C — MERGE LAB + VITAL + WAVEFORM\n",
    "=================\n",
    "\n",
    "df_lab_vit = lab_all.merge(vital_all, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"outer\").fillna(0)\n",
    "df_all = df_lab_vit.merge(df_wave, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"inner\")\n",
    "\n",
    "print(\"\\nMerged lab+vital+wave entries:\", df_all.shape[0])\n",
    "\n",
    "\n",
    "=================\n",
    "# STRICT Lab Overlap + STRICT Vital Overlap\n",
    "=================\n",
    "\n",
    "print(\"\\n=== Computing STRICT Lab–Waveform and Vital–Waveform Overlap ===\")\n",
    "\n",
    "curr_lab_by_hadm = df_curr_lab.groupby([\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "curr_vit_by_hadm = df_curr_vit.groupby([\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "\n",
    "lab_overlap = []\n",
    "vit_overlap = []\n",
    "\n",
    "for _, row in df_all.iterrows():\n",
    "\n",
    "    subj = row[\"SUBJECT_ID\"]\n",
    "    hadm = row[\"HADM_ID\"]\n",
    "\n",
    "    w_start = row[\"wave_start_dt\"]\n",
    "    w_end   = row[\"wave_end_dt\"]\n",
    "\n",
    "    key = (subj, hadm)\n",
    "\n",
    "    # LAB\n",
    "    if key in curr_lab_by_hadm.groups:\n",
    "        labs = curr_lab_by_hadm.get_group(key)\n",
    "        times = labs[\"CHARTTIME\"].dropna().sort_values().to_numpy()\n",
    "        inside = (times >= w_start.to_datetime64()) & (times <= w_end.to_datetime64())\n",
    "        lab_overlap.append(float(inside.sum() / len(times)) if len(times) else 0.0)\n",
    "    else:\n",
    "        lab_overlap.append(0.0)\n",
    "\n",
    "    # VITAL\n",
    "    if key in curr_vit_by_hadm.groups:\n",
    "        vs = curr_vit_by_hadm.get_group(key)\n",
    "        vt = vs[\"CHARTTIME\"].dropna().sort_values().to_numpy()\n",
    "        inside = (vt >= w_start.to_datetime64()) & (vt <= w_end.to_datetime64())\n",
    "        vit_overlap.append(float(inside.sum() / len(vt)) if len(vt) else 0.0)\n",
    "    else:\n",
    "        vit_overlap.append(0.0)\n",
    "\n",
    "df_all[\"lab_overlap_fraction\"] = lab_overlap\n",
    "df_all[\"vital_overlap_fraction\"] = vit_overlap\n",
    "\n",
    "\n",
    "=================\n",
    "# VITAL TIME COVERAGE (NEW)\n",
    "=================\n",
    "\n",
    "# Count total vitals (current only)\n",
    "vital_counts = (\n",
    "    df_curr_vit.groupby([\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "               .size()\n",
    "               .reset_index(name=\"vital_count\")\n",
    ")\n",
    "\n",
    "df_all = df_all.merge(vital_counts, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"left\")\n",
    "df_all[\"vital_count\"] = df_all[\"vital_count\"].fillna(0)\n",
    "\n",
    "df_all[\"wave_hours\"] = (df_all[\"wave_end_dt\"] - df_all[\"wave_start_dt\"]).dt.total_seconds() / 3600.0\n",
    "df_all[\"wave_hours\"] = df_all[\"wave_hours\"].clip(lower=1e-6)\n",
    "\n",
    "df_all[\"vital_per_4hr\"] = (df_all[\"vital_count\"] / df_all[\"wave_hours\"]) * 4\n",
    "\n",
    "\n",
    "=================\n",
    "# FINAL QUALITY FILTER (UPDATED)\n",
    "=================\n",
    "\n",
    "print(\"\\n=== Selecting HIGH-QUALITY data (labs + vitals + waveform) ===\")\n",
    "\n",
    "df_quality = df_all[\n",
    "    (df_all[\"curr_lab_types_dense\"]   >= 4)  &\n",
    "    #(df_all[\"curr_vital_types_dense\"] >= 3)  &\n",
    "    (df_all[\"nseg\"] >= 2000)                 &\n",
    "    #(df_all[\"vital_overlap_fraction\"] >= 0.90) &\n",
    "    (df_all[\"vital_per_4hr\"] >= 1)             # NEW coverage filter\n",
    "]\n",
    "\n",
    "print(\"\\nHigh-quality entries:\", df_quality.shape[0])\n",
    "\n",
    "\n",
    "=================\n",
    "# GENERATE OUTPUT JSON\n",
    "=================\n",
    "\n",
    "info_list = []\n",
    "for _, row in df_quality.iterrows():\n",
    "    fname = row[\"file\"]\n",
    "    hadm  = int(row[\"HADM_ID\"])\n",
    "    nseg  = int(row[\"nseg\"])\n",
    "\n",
    "    entry = [\n",
    "        fname,\n",
    "        hadm,\n",
    "        0,\n",
    "        nseg - 1,\n",
    "        nseg - 1,\n",
    "        0\n",
    "    ]\n",
    "    info_list.append(entry)\n",
    "\n",
    "with open(OUTPUT_LIST, \"w\") as f:\n",
    "    json.dump(info_list, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved\", len(info_list), \"entries to\", OUTPUT_LIST)\n",
    "if len(info_list):\n",
    "    print(\"Example entry:\", info_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc715c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSVs:\n",
      " demo   : (6737, 11)\n",
      " hist   : (640371, 9)\n",
      " target : (79383, 13)\n",
      " vital_raw  : (845265, 14)\n",
      "Unique waveform encounters: 2694\n",
      "Joined:\n",
      " demo_enc   : (2692, 11)\n",
      " hist_enc   : (83309, 9)\n",
      " target_enc : (25648, 13)\n",
      " vital_stats: (2694, 13)\n",
      "Final summary shape: (2694, 48)\n",
      "shape: (5, 48)\n",
      "┌────────────┬─────────┬────────┬────────────────────────┬───────────┬──────────┬────────────────┬───────────┬──────────────┬────────────────────┬──────────────────┬─────────────────┬──────────────────┬──────────────────┬─────────────────────┬────────────────────┬──────────────────┬─────────────────┬──────────────────┬──────────────────┬─────────────────────┬──────────────────────┬────────────────────┬───────────────────┬────────────────────┬────────────────────┬───────────────────────┬───────────────────────┬─────────────────────┬────────────────────┬─────────────────────┬─────────────────────┬────────────────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────────┬─────────────────┬─────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────┬─────────┬─────────┬─────────────────────────────────┐\n",
      "│ SUBJECT_ID ┆ HADM_ID ┆ GENDER ┆ ETHNICITY              ┆ INSURANCE ┆ LANGUAGE ┆ MARITAL_STATUS ┆ ICD9_CODE ┆ age_at_admit ┆ Potassium_hist_min ┆ Calcium_hist_min ┆ Sodium_hist_min ┆ Glucose_hist_min ┆ Lactate_hist_min ┆ Creatinine_hist_min ┆ Potassium_hist_max ┆ Calcium_hist_max ┆ Sodium_hist_max ┆ Glucose_hist_max ┆ Lactate_hist_max ┆ Creatinine_hist_max ┆ Potassium_hist_range ┆ Calcium_hist_range ┆ Sodium_hist_range ┆ Glucose_hist_range ┆ Lactate_hist_range ┆ Creatinine_hist_range ┆ Potassium_target_mean ┆ Calcium_target_mean ┆ Sodium_target_mean ┆ Glucose_target_mean ┆ Lactate_target_mean ┆ Creatinine_target_mean ┆ ABPs_count ┆ ABPd_count ┆ ABPm_count ┆ NBPs_count ┆ NBPd_count ┆ NBPm_count ┆ NBP_total_count ┆ vital_start         ┆ vital_end           ┆ vital_hours ┆ NBP_per_hour ┆ gender_bin ┆ age_bin ┆ NBP_bin ┆ strata                          │\n",
      "│ ---        ┆ ---     ┆ ---    ┆ ---                    ┆ ---       ┆ ---      ┆ ---            ┆ ---       ┆ ---          ┆ ---                ┆ ---              ┆ ---             ┆ ---              ┆ ---              ┆ ---                 ┆ ---                ┆ ---              ┆ ---             ┆ ---              ┆ ---              ┆ ---                 ┆ ---                  ┆ ---                ┆ ---               ┆ ---                ┆ ---                ┆ ---                   ┆ ---                   ┆ ---                 ┆ ---                ┆ ---                 ┆ ---                 ┆ ---                    ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---             ┆ ---                 ┆ ---                 ┆ ---         ┆ ---          ┆ ---        ┆ ---     ┆ ---     ┆ ---                             │\n",
      "│ i64        ┆ i64     ┆ str    ┆ str                    ┆ str       ┆ str      ┆ str            ┆ str       ┆ f64          ┆ f64                ┆ f64              ┆ f64             ┆ f64              ┆ f64              ┆ f64                 ┆ f64                ┆ f64              ┆ f64             ┆ f64              ┆ f64              ┆ f64                 ┆ f64                  ┆ f64                ┆ f64               ┆ f64                ┆ f64                ┆ f64                   ┆ f64                   ┆ f64                 ┆ f64                ┆ f64                 ┆ f64                 ┆ f64                    ┆ u32        ┆ u32        ┆ u32        ┆ u32        ┆ u32        ┆ u32        ┆ u32             ┆ datetime[μs]        ┆ datetime[μs]        ┆ f64         ┆ f64          ┆ i32        ┆ i32     ┆ i32     ┆ str                             │\n",
      "╞════════════╪═════════╪════════╪════════════════════════╪═══════════╪══════════╪════════════════╪═══════════╪══════════════╪════════════════════╪══════════════════╪═════════════════╪══════════════════╪══════════════════╪═════════════════════╪════════════════════╪══════════════════╪═════════════════╪══════════════════╪══════════════════╪═════════════════════╪══════════════════════╪════════════════════╪═══════════════════╪════════════════════╪════════════════════╪═══════════════════════╪═══════════════════════╪═════════════════════╪════════════════════╪═════════════════════╪═════════════════════╪════════════════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═════════════════╪═════════════════════╪═════════════════════╪═════════════╪══════════════╪════════════╪═════════╪═════════╪═════════════════════════════════╡\n",
      "│ 107        ┆ 174162  ┆ M      ┆ HISPANIC OR LATINO     ┆ Medicare  ┆ ENGL     ┆ SEPARATED      ┆ 53291     ┆ 70.113121    ┆ 3.7                ┆ 7.1              ┆ 135.0           ┆ 79.0             ┆ 0.0              ┆ 5.3                 ┆ 6.1                ┆ 7.8              ┆ 139.0           ┆ 162.0            ┆ 0.0              ┆ 9.0                 ┆ 2.4                  ┆ 0.7                ┆ 4.0               ┆ 83.0               ┆ 0.0                ┆ 3.7                   ┆ 4.7                   ┆ 7.75                ┆ 137.5              ┆ 120.5               ┆ NaN                 ┆ 7.05                   ┆ 0          ┆ 0          ┆ 0          ┆ 50         ┆ 50         ┆ 52         ┆ 152             ┆ 2122-05-14 21:08:00 ┆ 2122-05-16 17:00:00 ┆ 43.866667   ┆ 3.465046     ┆ 0          ┆ 4       ┆ 4       ┆ 4_0_HISPANIC OR LATINO_Medicar… │\n",
      "│ 107        ┆ 182383  ┆ M      ┆ HISPANIC OR LATINO     ┆ Medicare  ┆ ENGL     ┆ SEPARATED      ┆ 99681     ┆ 69.661351    ┆ 3.3                ┆ 7.9              ┆ 137.0           ┆ 90.0             ┆ 1.8              ┆ 5.8                 ┆ 8.2                ┆ 9.3              ┆ 143.0           ┆ 155.0            ┆ 1.8              ┆ 12.1                ┆ 4.9                  ┆ 1.4                ┆ 6.0               ┆ 65.0               ┆ 0.0                ┆ 6.3                   ┆ 3.9                   ┆ 8.65                ┆ 139.5              ┆ 107.5               ┆ NaN                 ┆ 7.4                    ┆ 0          ┆ 0          ┆ 0          ┆ 26         ┆ 26         ┆ 26         ┆ 78              ┆ 2121-11-30 20:05:00 ┆ 2121-12-01 21:00:00 ┆ 24.916667   ┆ 3.130435     ┆ 0          ┆ 3       ┆ 4       ┆ 3_0_HISPANIC OR LATINO_Medicar… │\n",
      "│ 109        ┆ 126055  ┆ F      ┆ BLACK/AFRICAN AMERICAN ┆ Medicaid  ┆ ENGL     ┆ SINGLE         ┆ 4010      ┆ 24.186079    ┆ 3.9                ┆ 6.4              ┆ 130.0           ┆ 75.0             ┆ 0.6              ┆ 5.2                 ┆ 5.9                ┆ 8.4              ┆ 140.0           ┆ 123.0            ┆ 1.3              ┆ 9.2                 ┆ 2.0                  ┆ 2.0                ┆ 10.0              ┆ 48.0               ┆ 0.7                ┆ 4.0                   ┆ 4.58                  ┆ 7.56                ┆ 135.0              ┆ 104.166667          ┆ NaN                 ┆ 7.083333               ┆ 0          ┆ 0          ┆ 0          ┆ 128        ┆ 128        ┆ 125        ┆ 381             ┆ 2141-10-21 02:03:00 ┆ 2141-10-25 20:00:00 ┆ 113.95      ┆ 3.343572     ┆ 1          ┆ 1       ┆ 4       ┆ 1_1_BLACK/AFRICAN AMERICAN_Med… │\n",
      "│ 109        ┆ 155726  ┆ F      ┆ BLACK/AFRICAN AMERICAN ┆ Medicaid  ┆ ENGL     ┆ SINGLE         ┆ 40301     ┆ 25.016205    ┆ 4.8                ┆ 0.0              ┆ 137.0           ┆ 80.0             ┆ 0.0              ┆ 5.8                 ┆ 4.8                ┆ 0.0              ┆ 137.0           ┆ 80.0             ┆ 0.0              ┆ 5.8                 ┆ 0.0                  ┆ 0.0                ┆ 0.0               ┆ 0.0                ┆ 0.0                ┆ 0.0                   ┆ 4.75                  ┆ 8.25                ┆ 136.5              ┆ 109.0               ┆ 0.9                 ┆ 6.65                   ┆ 0          ┆ 0          ┆ 0          ┆ 40         ┆ 40         ┆ 40         ┆ 120             ┆ 2142-08-13 06:02:00 ┆ 2142-08-14 21:00:00 ┆ 38.966667   ┆ 3.079555     ┆ 1          ┆ 1       ┆ 3       ┆ 1_1_BLACK/AFRICAN AMERICAN_Med… │\n",
      "│ 154        ┆ 102354  ┆ M      ┆ WHITE                  ┆ Medicare  ┆ ENGL     ┆ SEPARATED      ┆ 41081     ┆ 54.408712    ┆ 4.0                ┆ 8.1              ┆ 137.0           ┆ 107.0            ┆ 0.0              ┆ 0.9                 ┆ 4.4                ┆ 8.1              ┆ 138.0           ┆ 129.0            ┆ 0.0              ┆ 1.0                 ┆ 0.4                  ┆ 0.0                ┆ 1.0               ┆ 22.0               ┆ 0.0                ┆ 0.1                   ┆ 4.2                   ┆ 8.15                ┆ 139.5              ┆ 123.5               ┆ NaN                 ┆ 1.0                    ┆ 0          ┆ 0          ┆ 0          ┆ 50         ┆ 50         ┆ 50         ┆ 150             ┆ 2127-12-23 22:03:00 ┆ 2127-12-25 23:00:00 ┆ 48.95       ┆ 3.064351     ┆ 0          ┆ 3       ┆ 3       ┆ 3_0_WHITE_Medicare_ENGL_SEPARA… │\n",
      "└────────────┴─────────┴────────┴────────────────────────┴───────────┴──────────┴────────────────┴───────────┴──────────────┴────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┴─────────────────────┴────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┴─────────────────────┴──────────────────────┴────────────────────┴───────────────────┴────────────────────┴────────────────────┴───────────────────────┴───────────────────────┴─────────────────────┴────────────────────┴─────────────────────┴─────────────────────┴────────────────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────────┴─────────────────┴─────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────┴─────────┴─────────┴─────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/labs/hulab/mxwang/anaconda3/envs/S4M/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1023: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encounters: 2020\n",
      "Test encounters : 674\n",
      "Saved: ./ppg_split_lists_stratified_hadm_labdemo.json\n",
      "Train: 2020\n",
      "Test : 674\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_tbl_width_chars(2000)\n",
    "\n",
    "\n",
    "# 1. Paths / Config\n",
    "\n",
    "DEMO_PATH   = \"./mimic_patient_admission_demo_with_diag.csv\"\n",
    "HIST_PATH   = \"./mimic_wav_lab_history.csv\"\n",
    "TARGET_PATH = \"./mimic_wav_lab_overlap.csv\"\n",
    "VITAL_PATH  = \"./mimic_wav_vital_overlap.csv\"\n",
    "\n",
    "HQ_JSON     = \"./mimic_high_quality_info_list.json\"\n",
    "OUT_SPLIT_JSON = \"./ppg_split_lists_stratified_hadm_labdemo.json\"\n",
    "\n",
    "lab_cols   = [\"Potassium\", \"Calcium\", \"Sodium\", \"Glucose\", \"Lactate\", \"Creatinine\"]\n",
    "vital_cols = [\"ABPs\", \"ABPd\", \"ABPm\", \"NBPs\", \"NBPd\", \"NBPm\"]\n",
    "nbp_cols   = [\"NBPs\", \"NBPd\", \"NBPm\"]\n",
    "\n",
    "\n",
    "# 2. Load CSVs\n",
    "\n",
    "demo   = pl.read_csv(DEMO_PATH,   infer_schema_length=20000)\n",
    "hist   = pl.read_csv(HIST_PATH,   infer_schema_length=20000)\n",
    "target = pl.read_csv(TARGET_PATH, infer_schema_length=20000)\n",
    "\n",
    "# vital → string first, then convert using to_datetime\n",
    "vital_raw = pl.read_csv(VITAL_PATH, infer_schema_length=20000)\n",
    "\n",
    "print(\"Loaded CSVs:\")\n",
    "print(\" demo   :\", demo.shape)\n",
    "print(\" hist   :\", hist.shape)\n",
    "print(\" target :\", target.shape)\n",
    "print(\" vital_raw  :\", vital_raw.shape)\n",
    "\n",
    "\n",
    "# 3. Load HQ waveform encounters\n",
    "\n",
    "with open(HQ_JSON, \"r\") as f:\n",
    "    ppg_meta = json.load(f)\n",
    "\n",
    "encounters = []\n",
    "for entry in ppg_meta:\n",
    "    fname = entry[0]\n",
    "    parts = fname.split(\"_\")\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    try:\n",
    "        subj = int(parts[0])\n",
    "        hadm = int(parts[1])\n",
    "        encounters.append((subj, hadm))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "encounters = sorted(set(encounters))\n",
    "print(\"Unique waveform encounters:\", len(encounters))\n",
    "\n",
    "base_enc = pl.DataFrame({\n",
    "    \"SUBJECT_ID\": [p[0] for p in encounters],\n",
    "    \"HADM_ID\":    [p[1] for p in encounters],\n",
    "})\n",
    "\n",
    "\n",
    "# 4. Join encounters\n",
    "demo_enc   = demo.join(base_enc,   on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "hist_enc   = hist.join(base_enc,   on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "target_enc = target.join(base_enc, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "\n",
    "print(\"Joined:\")\n",
    "print(\" demo_enc   :\", demo_enc.shape)\n",
    "print(\" hist_enc   :\", hist_enc.shape)\n",
    "print(\" target_enc :\", target_enc.shape)\n",
    "\n",
    "\n",
    "# 5. Demographics\n",
    "demo_stats = (\n",
    "    demo_enc\n",
    "    .group_by([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    .agg([\n",
    "        pl.col(\"GENDER\").first().alias(\"GENDER\"),\n",
    "        pl.col(\"ETHNICITY\").first().alias(\"ETHNICITY\"),\n",
    "        pl.col(\"INSURANCE\").first().alias(\"INSURANCE\"),\n",
    "        pl.col(\"LANGUAGE\").first().alias(\"LANGUAGE\"),\n",
    "        pl.col(\"MARITAL_STATUS\").first().alias(\"MARITAL_STATUS\"),\n",
    "        pl.col(\"ICD9_CODE\").first().alias(\"ICD9_CODE\"),\n",
    "        pl.col(\"age_at_admit\").mean().alias(\"age_at_admit\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# 6. Lab history stats\n",
    "hist_stats = (\n",
    "    hist_enc\n",
    "    .select([\"SUBJECT_ID\", \"HADM_ID\"] + lab_cols)\n",
    "    .group_by([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    .agg(\n",
    "        [pl.col(c).min().alias(f\"{c}_hist_min\") for c in lab_cols] +\n",
    "        [pl.col(c).max().alias(f\"{c}_hist_max\") for c in lab_cols]\n",
    "    )\n",
    ")\n",
    "\n",
    "for lab in lab_cols:\n",
    "    hist_stats = hist_stats.with_columns(\n",
    "        (pl.col(f\"{lab}_hist_max\") - pl.col(f\"{lab}_hist_min\")).alias(f\"{lab}_hist_range\")\n",
    "    )\n",
    "\n",
    "\n",
    "# 7. Target mean\n",
    "target_stats = (\n",
    "    target_enc\n",
    "    .select([\"SUBJECT_ID\", \"HADM_ID\"] + lab_cols)\n",
    "    .group_by([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    .agg([pl.col(c).mean().alias(f\"{c}_target_mean\") for c in lab_cols])\n",
    ")\n",
    "\n",
    "\n",
    "# 8. Vital stats (NBP-based)\n",
    "vital = vital_raw.with_columns(\n",
    "    pl.col(\"CHARTTIME\").str.to_datetime(strict=False)\n",
    ")\n",
    "\n",
    "vital_enc = vital.join(base_enc, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"inner\")\n",
    "\n",
    "vital_stats = (\n",
    "    vital_enc\n",
    "    .select([\"SUBJECT_ID\", \"HADM_ID\"] + vital_cols)\n",
    "    .group_by([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    .agg([pl.col(c).count().alias(f\"{c}_count\") for c in vital_cols])\n",
    ")\n",
    "\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    sum(pl.col(f\"{c}_count\") for c in nbp_cols).alias(\"NBP_total_count\")\n",
    ")\n",
    "\n",
    "dur_df = (\n",
    "    vital_enc\n",
    "    .group_by([\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "    .agg([\n",
    "        pl.col(\"CHARTTIME\").min().alias(\"vital_start\"),\n",
    "        pl.col(\"CHARTTIME\").max().alias(\"vital_end\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "dur_df = dur_df.with_columns([\n",
    "    pl.col(\"vital_start\").cast(pl.Datetime),\n",
    "    pl.col(\"vital_end\").cast(pl.Datetime),\n",
    "])\n",
    "\n",
    "vital_stats = vital_stats.join(dur_df, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\")\n",
    "\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    ((pl.col(\"vital_end\") - pl.col(\"vital_start\")).dt.total_seconds().fill_null(0) / 3600.0)\n",
    "    .alias(\"vital_hours\")\n",
    ")\n",
    "\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    (pl.col(\"NBP_total_count\") / (pl.col(\"vital_hours\") + 1e-6)).alias(\"NBP_per_hour\")\n",
    ")\n",
    "\n",
    "print(\" vital_stats:\", vital_stats.shape)\n",
    "\n",
    "\n",
    "# 9. Merge\n",
    "summary = (\n",
    "    base_enc\n",
    "    .join(demo_stats,   on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\")\n",
    "    .join(hist_stats,   on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\")\n",
    "    .join(target_stats, on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\")\n",
    "    .join(vital_stats,  on=[\"SUBJECT_ID\", \"HADM_ID\"], how=\"left\")\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "summary = summary.with_columns([\n",
    "    pl.when(pl.col(\"GENDER\") == \"M\").then(0).otherwise(1).alias(\"gender_bin\"),\n",
    "    pl.when(pl.col(\"age_at_admit\") < 30).then(1)\n",
    "     .when(pl.col(\"age_at_admit\") < 50).then(2)\n",
    "     .when(pl.col(\"age_at_admit\") < 70).then(3)\n",
    "     .when(pl.col(\"age_at_admit\") < 90).then(4)\n",
    "     .otherwise(5)\n",
    "     .alias(\"age_bin\"),\n",
    "])\n",
    "\n",
    "\n",
    "# 10. Vital bins\n",
    "nbp_vals = summary[\"NBP_per_hour\"].to_numpy()\n",
    "finite = np.isfinite(nbp_vals)\n",
    "\n",
    "if finite.sum() > 10:\n",
    "    q = np.nanquantile(nbp_vals[finite], [0.2, 0.4, 0.6, 0.8])\n",
    "    summary = summary.with_columns(\n",
    "        pl.when(pl.col(\"NBP_per_hour\") < q[0]).then(0)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[1]).then(1)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[2]).then(2)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[3]).then(3)\n",
    "         .otherwise(4)\n",
    "         .alias(\"NBP_bin\")\n",
    "    )\n",
    "else:\n",
    "    summary = summary.with_columns(pl.lit(2).alias(\"NBP_bin\"))\n",
    "\n",
    "\n",
    "# 11. Final strata (now fully cleaned)\n",
    "combo_cols = [\n",
    "    \"age_bin\",\n",
    "    \"gender_bin\",\n",
    "    \"ETHNICITY\",\n",
    "    \"INSURANCE\",\n",
    "    \"LANGUAGE\",\n",
    "    \"MARITAL_STATUS\",\n",
    "    \"ICD9_CODE\",\n",
    "    \"NBP_bin\",\n",
    "]\n",
    "\n",
    "def combine_fields(df, cols):\n",
    "    out = df[cols[0]].cast(pl.Utf8)\n",
    "    for c in cols[1:]:\n",
    "        out = out + \"_\" + df[c].cast(pl.Utf8)\n",
    "    return out\n",
    "\n",
    "summary = summary.with_columns(\n",
    "    combine_fields(summary, combo_cols).alias(\"strata\")\n",
    ")\n",
    "\n",
    "# ---- CLEAN categorical values for sklearn ----\n",
    "categorical_cols = [\n",
    "    \"ETHNICITY\", \"INSURANCE\", \"LANGUAGE\",\n",
    "    \"MARITAL_STATUS\", \"ICD9_CODE\", \"strata\"\n",
    "]\n",
    "\n",
    "summary = summary.with_columns([\n",
    "    pl.col(c).cast(pl.Utf8).fill_null(\"UNK\") for c in categorical_cols\n",
    "])\n",
    "\n",
    "summary_pd = summary.to_pandas()\n",
    "summary_pd[\"strata\"] = summary_pd[\"strata\"].astype(str)\n",
    "\n",
    "print(\"Final summary shape:\", summary.shape)\n",
    "print(summary.head(5))\n",
    "\n",
    "\n",
    "# 12. StratifiedGroupKFold\n",
    "hadm_ids = summary_pd[\"HADM_ID\"].to_numpy()\n",
    "strata   = summary_pd[\"strata\"].to_numpy()\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "train_idx, test_idx = next(sgkf.split(hadm_ids, strata, groups=hadm_ids))\n",
    "\n",
    "train_hadm = set(hadm_ids[train_idx])\n",
    "test_hadm  = set(hadm_ids[test_idx])\n",
    "\n",
    "print(\"Train encounters:\", len(train_hadm))\n",
    "print(\"Test encounters :\", len(test_hadm))\n",
    "\n",
    "\n",
    "# 13. Assign waveform files\n",
    "split_dict = {\n",
    "    \"train_control_list\": [],\n",
    "    \"test_control_list\": [],\n",
    "}\n",
    "\n",
    "for entry in ppg_meta:\n",
    "    hadm = int(entry[1])\n",
    "    if hadm in train_hadm:\n",
    "        split_dict[\"train_control_list\"].append(entry)\n",
    "    elif hadm in test_hadm:\n",
    "        split_dict[\"test_control_list\"].append(entry)\n",
    "\n",
    "with open(OUT_SPLIT_JSON, \"w\") as f:\n",
    "    json.dump(split_dict, f, indent=4)\n",
    "\n",
    "print(\"Saved:\", OUT_SPLIT_JSON)\n",
    "print(\"Train:\", len(split_dict[\"train_control_list\"]))\n",
    "print(\"Test :\", len(split_dict[\"test_control_list\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S4M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
