{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea119fda",
   "metadata": {},
   "source": [
    "# find the high quality subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "# CONFIGURATION\n",
    "#\n",
    "WAVEFORM_DIR = \"/opt/localdata100tb/UNIPHY_Plus/dataset/EST/MIMIC3_SPO2_I_40hz_v3\"\n",
    "OUTPUT_LIST = \"mimic_high_quality_info_list.json\"\n",
    "\n",
    "lab_cols = [\n",
    "    \"Potassium\",\"Calcium\",\"Sodium\",\"Glucose\",\n",
    "    \"Lactate\",\"Creatinine\"\n",
    "]\n",
    "\n",
    "vital_cols = [\n",
    "    \"NBPs\",\"NBPd\",\"NBPm\"\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE A — LOAD COMBINED LAB + VITAL TABLES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Loading combined lab+vital tables ===\")\n",
    "\n",
    "df_curr = pd.read_csv(\"mimic_lab_vital_waveform_overlap.csv\", parse_dates=[\"CHARTTIME\"])\n",
    "df_hist = pd.read_csv(\"mimic_lab_vital_waveform_history.csv\", parse_dates=[\"CHARTTIME\"])\n",
    "\n",
    "#\n",
    "# ---------- LAB summary ----------\n",
    "#\n",
    "df_curr_lab_summary = (\n",
    "    df_curr.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[lab_cols]\n",
    "           .count()\n",
    "           .reset_index()\n",
    ")\n",
    "df_curr_lab_summary = df_curr_lab_summary.rename(\n",
    "    columns={c: f\"{c}_curr\" for c in lab_cols}\n",
    ")\n",
    "\n",
    "df_hist_lab_summary = (\n",
    "    df_hist.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[lab_cols]\n",
    "           .count()\n",
    "           .reset_index()\n",
    ")\n",
    "df_hist_lab_summary = df_hist_lab_summary.rename(\n",
    "    columns={c: f\"{c}_hist\" for c in lab_cols}\n",
    ")\n",
    "\n",
    "lab_all = df_curr_lab_summary.merge(\n",
    "    df_hist_lab_summary,\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"outer\"\n",
    ").fillna(0)\n",
    "\n",
    "lab_all[\"curr_lab_types_present\"] = (\n",
    "    lab_all[[f\"{c}_curr\" for c in lab_cols]] > 0\n",
    ").sum(axis=1)\n",
    "\n",
    "lab_all[\"curr_lab_types_dense\"] = (\n",
    "    lab_all[[f\"{c}_curr\" for c in lab_cols]] >= 2\n",
    ").sum(axis=1)\n",
    "\n",
    "#\n",
    "# ---------- VITAL summary ----------\n",
    "#\n",
    "df_curr_vit_summary = (\n",
    "    df_curr.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[vital_cols]\n",
    "           .count()\n",
    "           .reset_index()\n",
    ")\n",
    "df_curr_vit_summary = df_curr_vit_summary.rename(\n",
    "    columns={c: f\"{c}_curr_v\" for c in vital_cols}\n",
    ")\n",
    "\n",
    "df_hist_vit_summary = (\n",
    "    df_hist.groupby([\"SUBJECT_ID\",\"HADM_ID\"])[vital_cols]\n",
    "           .count()\n",
    "           .reset_index()\n",
    ")\n",
    "df_hist_vit_summary = df_hist_vit_summary.rename(\n",
    "    columns={c: f\"{c}_hist_v\" for c in vital_cols}\n",
    ")\n",
    "\n",
    "vital_all = df_curr_vit_summary.merge(\n",
    "    df_hist_vit_summary,\n",
    "    on=[\"SUBJECT_ID\",\"HADM_ID\"],\n",
    "    how=\"outer\"\n",
    ").fillna(0)\n",
    "\n",
    "vital_all[\"curr_vital_types_present\"] = (\n",
    "    vital_all[[f\"{c}_curr_v\" for c in vital_cols]] > 0\n",
    ").sum(axis=1)\n",
    "\n",
    "vital_all[\"curr_vital_types_dense\"] = (\n",
    "    vital_all[[f\"{c}_curr_v\" for c in vital_cols]] >= 3\n",
    ").sum(axis=1)\n",
    "\n",
    "print(\"Total SUBJECT/HADM with lab+vital data:\", len(lab_all))\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE B — LOAD WAVEFORM METADATA FROM NPZ FILES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Scanning waveform files ===\")\n",
    "\n",
    "wave_recs = []\n",
    "files = [f for f in os.listdir(WAVEFORM_DIR) if f.endswith(\".npz\")]\n",
    "print(\"Total files in waveform dir:\", len(files))\n",
    "\n",
    "for fname in files:\n",
    "    base = fname[:-4]\n",
    "    parts = base.split(\"_\")\n",
    "    if len(parts) < 7:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        subj_id = int(parts[0])\n",
    "        hadm_id = int(parts[1])\n",
    "        clip_str = parts[2]\n",
    "        nseg = int(parts[-1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        clip_raw_index = [int(x) for x in clip_str.split(\"-\")]\n",
    "    except:\n",
    "        clip_raw_index = []\n",
    "\n",
    "    arr = np.load(os.path.join(WAVEFORM_DIR, fname), allow_pickle=True)\n",
    "    if \"time\" not in arr:\n",
    "        continue\n",
    "\n",
    "    time_ms = arr[\"time\"]\n",
    "    if len(time_ms) == 0:\n",
    "        continue\n",
    "\n",
    "    wave_start_dt = pd.to_datetime(time_ms[0], unit=\"ms\", errors=\"coerce\")\n",
    "    wave_end_dt   = pd.to_datetime(time_ms[-1] + 30000, unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "    wave_recs.append((\n",
    "        subj_id,\n",
    "        hadm_id,\n",
    "        fname,\n",
    "        nseg,\n",
    "        wave_start_dt,\n",
    "        wave_end_dt,\n",
    "        clip_raw_index\n",
    "    ))\n",
    "\n",
    "df_wave = pd.DataFrame(\n",
    "    wave_recs,\n",
    "    columns=[\n",
    "        \"SUBJECT_ID\",\"HADM_ID\",\"file\",\"nseg\",\n",
    "        \"wave_start_dt\",\"wave_end_dt\",\"clip_raw_index\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Parsed waveform entries:\", df_wave.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE C — MERGE LAB + VITAL + WAVEFORM\n",
    "# ============================================================\n",
    "\n",
    "df_lab_vit = lab_all.merge(vital_all, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"outer\").fillna(0)\n",
    "df_all = df_lab_vit.merge(df_wave, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"inner\")\n",
    "\n",
    "print(\"\\nMerged lab+vital+wave entries:\", df_all.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# STRICT LAB + VITAL OVERLAP FRACTIONS\n",
    "print(\"\\n=== Computing STRICT Lab–Waveform and Vital–Waveform Overlap ===\")\n",
    "\n",
    "curr_lab_by_hadm = df_curr.groupby([\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "curr_vit_by_hadm = df_curr.groupby([\"SUBJECT_ID\",\"HADM_ID\"])  # vitals also in df_curr\n",
    "\n",
    "lab_overlap = []\n",
    "vit_overlap = []\n",
    "\n",
    "for _, row in df_all.iterrows():\n",
    "\n",
    "    subj = row[\"SUBJECT_ID\"]\n",
    "    hadm = row[\"HADM_ID\"]\n",
    "\n",
    "    w_start = row[\"wave_start_dt\"]\n",
    "    w_end   = row[\"wave_end_dt\"]\n",
    "\n",
    "    key = (subj, hadm)\n",
    "\n",
    "    # LAB strict overlap\n",
    "    if key in curr_lab_by_hadm.groups:\n",
    "        labs = curr_lab_by_hadm.get_group(key)\n",
    "        times = labs[\"CHARTTIME\"].dropna().sort_values().to_numpy()\n",
    "        inside = (times >= w_start.to_datetime64()) & (times <= w_end.to_datetime64())\n",
    "        lab_overlap.append(float(inside.sum() / len(times)) if len(times) else 0.0)\n",
    "    else:\n",
    "        lab_overlap.append(0.0)\n",
    "\n",
    "    # VITAL strict overlap\n",
    "    if key in curr_vit_by_hadm.groups:\n",
    "        vs = curr_vit_by_hadm.get_group(key)\n",
    "        vt = vs[\"CHARTTIME\"].dropna().sort_values().to_numpy()\n",
    "        inside = (vt >= w_start.to_datetime64()) & (vt <= w_end.to_datetime64())\n",
    "        vit_overlap.append(float(inside.sum() / len(vt)) if len(vt) else 0.0)\n",
    "    else:\n",
    "        vit_overlap.append(0.0)\n",
    "\n",
    "df_all[\"lab_overlap_fraction\"] = lab_overlap\n",
    "df_all[\"vital_overlap_fraction\"] = vit_overlap\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VITAL TIME COVERAGE\n",
    "# ============================================================\n",
    "\n",
    "vital_counts = (\n",
    "    df_curr.groupby([\"SUBJECT_ID\",\"HADM_ID\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"vital_count\")\n",
    ")\n",
    "\n",
    "df_all = df_all.merge(vital_counts, on=[\"SUBJECT_ID\",\"HADM_ID\"], how=\"left\")\n",
    "df_all[\"vital_count\"] = df_all[\"vital_count\"].fillna(0)\n",
    "\n",
    "df_all[\"wave_hours\"] = (df_all[\"wave_end_dt\"] - df_all[\"wave_start_dt\"]).dt.total_seconds() / 3600.0\n",
    "df_all[\"wave_hours\"] = df_all[\"wave_hours\"].clip(lower=1e-6)\n",
    "\n",
    "df_all[\"vital_per_4hr\"] = (df_all[\"vital_count\"] / df_all[\"wave_hours\"]) * 4\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FINAL QUALITY FILTER\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n=== Selecting HIGH-QUALITY data (labs + vitals + waveform) ===\")\n",
    "\n",
    "df_quality = df_all[\n",
    "    (df_all[\"curr_lab_types_dense\"] >= 3) &\n",
    "    (df_all[\"nseg\"] >= 1800) &\n",
    "    (df_all[\"vital_per_4hr\"] >= 1)\n",
    "]\n",
    "\n",
    "print(\"\\nHigh-quality entries:\", df_quality.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OUTPUT JSON LIST\n",
    "# ============================================================\n",
    "\n",
    "info_list = []\n",
    "for _, row in df_quality.iterrows():\n",
    "    fname = row[\"file\"]\n",
    "    hadm  = int(row[\"HADM_ID\"])\n",
    "    nseg  = int(row[\"nseg\"])\n",
    "\n",
    "    entry = [fname, hadm, 0, nseg - 1, nseg - 1, 0]\n",
    "    info_list.append(entry)\n",
    "\n",
    "with open(OUTPUT_LIST, \"w\") as f:\n",
    "    json.dump(info_list, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved\", len(info_list), \"entries to\", OUTPUT_LIST)\n",
    "if len(info_list):\n",
    "    print(\"Example entry:\", info_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc715c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSVs:\n",
      " demo   : (6768, 11)\n",
      " hist   : (592971, 14)\n",
      " target : (443944, 16)\n",
      "Unique waveform encounters: 2608\n",
      "Joined:\n",
      " demo_enc   : (2567, 11)\n",
      " hist_enc   : (150924, 14)\n",
      " target_enc : (199257, 16)\n",
      " vital_stats: (2608, 10)\n",
      "Final summary shape: (2608, 45)\n",
      "shape: (5, 45)\n",
      "┌────────────┬─────────┬────────┬───────────┬───────────┬──────────┬────────────────┬───────────┬──────────────┬────────────────────┬──────────────────┬─────────────────┬──────────────────┬──────────────────┬─────────────────────┬────────────────────┬──────────────────┬─────────────────┬──────────────────┬──────────────────┬─────────────────────┬──────────────────────┬────────────────────┬───────────────────┬────────────────────┬────────────────────┬───────────────────────┬───────────────────────┬─────────────────────┬────────────────────┬─────────────────────┬─────────────────────┬────────────────────────┬────────────┬────────────┬────────────┬─────────────────┬─────────────────────┬─────────────────────┬─────────────┬──────────────┬────────────┬─────────┬─────────┬─────────────────────────────────┐\n",
      "│ SUBJECT_ID ┆ HADM_ID ┆ GENDER ┆ ETHNICITY ┆ INSURANCE ┆ LANGUAGE ┆ MARITAL_STATUS ┆ ICD9_CODE ┆ age_at_admit ┆ Potassium_hist_min ┆ Calcium_hist_min ┆ Sodium_hist_min ┆ Glucose_hist_min ┆ Lactate_hist_min ┆ Creatinine_hist_min ┆ Potassium_hist_max ┆ Calcium_hist_max ┆ Sodium_hist_max ┆ Glucose_hist_max ┆ Lactate_hist_max ┆ Creatinine_hist_max ┆ Potassium_hist_range ┆ Calcium_hist_range ┆ Sodium_hist_range ┆ Glucose_hist_range ┆ Lactate_hist_range ┆ Creatinine_hist_range ┆ Potassium_target_mean ┆ Calcium_target_mean ┆ Sodium_target_mean ┆ Glucose_target_mean ┆ Lactate_target_mean ┆ Creatinine_target_mean ┆ NBPs_count ┆ NBPd_count ┆ NBPm_count ┆ NBP_total_count ┆ vital_start         ┆ vital_end           ┆ vital_hours ┆ NBP_per_hour ┆ gender_bin ┆ age_bin ┆ NBP_bin ┆ strata                          │\n",
      "│ ---        ┆ ---     ┆ ---    ┆ ---       ┆ ---       ┆ ---      ┆ ---            ┆ ---       ┆ ---          ┆ ---                ┆ ---              ┆ ---             ┆ ---              ┆ ---              ┆ ---                 ┆ ---                ┆ ---              ┆ ---             ┆ ---              ┆ ---              ┆ ---                 ┆ ---                  ┆ ---                ┆ ---               ┆ ---                ┆ ---                ┆ ---                   ┆ ---                   ┆ ---                 ┆ ---                ┆ ---                 ┆ ---                 ┆ ---                    ┆ ---        ┆ ---        ┆ ---        ┆ ---             ┆ ---                 ┆ ---                 ┆ ---         ┆ ---          ┆ ---        ┆ ---     ┆ ---     ┆ ---                             │\n",
      "│ str        ┆ str     ┆ str    ┆ str       ┆ str       ┆ str      ┆ str            ┆ str       ┆ f64          ┆ f64                ┆ f64              ┆ f64             ┆ f64              ┆ f64              ┆ f64                 ┆ f64                ┆ f64              ┆ f64             ┆ f64              ┆ f64              ┆ f64                 ┆ f64                  ┆ f64                ┆ f64               ┆ f64                ┆ f64                ┆ f64                   ┆ f64                   ┆ f64                 ┆ f64                ┆ f64                 ┆ f64                 ┆ f64                    ┆ u32        ┆ u32        ┆ u32        ┆ u32             ┆ datetime[μs]        ┆ datetime[μs]        ┆ f64         ┆ f64          ┆ i32        ┆ i32     ┆ i32     ┆ str                             │\n",
      "╞════════════╪═════════╪════════╪═══════════╪═══════════╪══════════╪════════════════╪═══════════╪══════════════╪════════════════════╪══════════════════╪═════════════════╪══════════════════╪══════════════════╪═════════════════════╪════════════════════╪══════════════════╪═════════════════╪══════════════════╪══════════════════╪═════════════════════╪══════════════════════╪════════════════════╪═══════════════════╪════════════════════╪════════════════════╪═══════════════════════╪═══════════════════════╪═════════════════════╪════════════════════╪═════════════════════╪═════════════════════╪════════════════════════╪════════════╪════════════╪════════════╪═════════════════╪═════════════════════╪═════════════════════╪═════════════╪══════════════╪════════════╪═════════╪═════════╪═════════════════════════════════╡\n",
      "│ 10257      ┆ 178509  ┆ M      ┆ WHITE     ┆ Private   ┆ ENGL     ┆ MARRIED        ┆ 41401     ┆ 63.612423    ┆ 0.0                ┆ 0.0              ┆ 0.0             ┆ 0.0              ┆ 2.6              ┆ 0.0                 ┆ 0.0                ┆ 0.0              ┆ 0.0             ┆ 0.0              ┆ 3.6              ┆ 0.0                 ┆ 0.0                  ┆ 0.0                ┆ 0.0               ┆ 0.0                ┆ 1.0                ┆ 0.0                   ┆ 3.7                   ┆ 8.2                 ┆ 140.333333         ┆ 85.5                ┆ 2.4                 ┆ 0.633333               ┆ 32         ┆ 32         ┆ 32         ┆ 96              ┆ 2119-01-13 18:50:00 ┆ 2119-01-15 14:00:00 ┆ 43.166667   ┆ 2.223938     ┆ 0          ┆ 3       ┆ 1       ┆ 3_0_WHITE_Private_ENGL_MARRIED… │\n",
      "│ 10425      ┆ 147243  ┆ M      ┆ WHITE     ┆ Private   ┆ ENGL     ┆ MARRIED        ┆ 99686     ┆ 60.946492    ┆ 3.0                ┆ 5.5              ┆ 141.0           ┆ 81.0             ┆ 0.5              ┆ 1.1                 ┆ 6.7                ┆ 11.3             ┆ 152.0           ┆ 313.0            ┆ 8.8              ┆ 4.7                 ┆ 3.7                  ┆ 5.8                ┆ 11.0              ┆ 232.0              ┆ 8.3                ┆ 3.6                   ┆ 4.011111              ┆ 8.688889            ┆ 140.444444         ┆ 108.777778          ┆ 1.0                 ┆ 2.722222               ┆ 156        ┆ 156        ┆ 154        ┆ 466             ┆ 2199-08-29 13:05:00 ┆ 2199-09-04 17:00:00 ┆ 147.916667  ┆ 3.150423     ┆ 0          ┆ 3       ┆ 4       ┆ 3_0_WHITE_Private_ENGL_MARRIED… │\n",
      "│ 10532      ┆ 130690  ┆ M      ┆ WHITE     ┆ Medicare  ┆ ENGL     ┆ WIDOWED        ┆ 41189     ┆ 74.04495     ┆ 3.1                ┆ 7.8              ┆ 138.0           ┆ 91.0             ┆ 1.5              ┆ 1.0                 ┆ 4.4                ┆ 9.0              ┆ 139.0           ┆ 150.0            ┆ 1.9              ┆ 1.1                 ┆ 1.3                  ┆ 1.2                ┆ 1.0               ┆ 59.0               ┆ 0.4                ┆ 0.1                   ┆ 4.45                  ┆ 7.5                 ┆ 138.0              ┆ 104.5               ┆ 0.7                 ┆ 1.1                    ┆ 37         ┆ 37         ┆ 36         ┆ 110             ┆ 2132-02-11 10:00:00 ┆ 2132-02-13 00:01:00 ┆ 38.016667   ┆ 2.893468     ┆ 0          ┆ 4       ┆ 2       ┆ 4_0_WHITE_Medicare_ENGL_WIDOWE… │\n",
      "│ 10581      ┆ 123247  ┆ M      ┆ WHITE     ┆ Private   ┆ ENGL     ┆ WIDOWED        ┆ 5070      ┆ 82.433354    ┆ 4.0                ┆ 0.0              ┆ 132.0           ┆ 93.0             ┆ 1.7              ┆ 1.7                 ┆ 4.0                ┆ 0.0              ┆ 132.0           ┆ 93.0             ┆ 1.7              ┆ 1.7                 ┆ 0.0                  ┆ 0.0                ┆ 0.0               ┆ 0.0                ┆ 0.0                ┆ 0.0                   ┆ 3.9                   ┆ 8.05                ┆ 136.0              ┆ 154.0               ┆ 3.833333            ┆ 1.45                   ┆ 44         ┆ 44         ┆ 44         ┆ 132             ┆ 2135-12-21 23:24:00 ┆ 2135-12-23 17:00:00 ┆ 41.6        ┆ 3.173077     ┆ 0          ┆ 4       ┆ 4       ┆ 4_0_WHITE_Private_ENGL_WIDOWED… │\n",
      "│ 10694      ┆ 138159  ┆ F      ┆ WHITE     ┆ Medicare  ┆ ENGL     ┆ MARRIED        ┆ 03842     ┆ 88.503419    ┆ 4.5                ┆ 7.0              ┆ 138.0           ┆ 180.0            ┆ 4.1              ┆ 3.7                 ┆ 4.5                ┆ 7.0              ┆ 138.0           ┆ 180.0            ┆ 5.3              ┆ 4.2                 ┆ 0.0                  ┆ 0.0                ┆ 0.0               ┆ 0.0                ┆ 1.2                ┆ 0.5                   ┆ 3.830435              ┆ 8.477273            ┆ 140.913043         ┆ 180.857143          ┆ 2.8                 ┆ 2.731818               ┆ 249        ┆ 249        ┆ 248        ┆ 746             ┆ 2153-08-11 22:10:00 ┆ 2153-08-24 03:07:00 ┆ 292.95      ┆ 2.54651      ┆ 1          ┆ 4       ┆ 2       ┆ 4_1_WHITE_Medicare_ENGL_MARRIE… │\n",
      "└────────────┴─────────┴────────┴───────────┴───────────┴──────────┴────────────────┴───────────┴──────────────┴────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┴─────────────────────┴────────────────────┴──────────────────┴─────────────────┴──────────────────┴──────────────────┴─────────────────────┴──────────────────────┴────────────────────┴───────────────────┴────────────────────┴────────────────────┴───────────────────────┴───────────────────────┴─────────────────────┴────────────────────┴─────────────────────┴─────────────────────┴────────────────────────┴────────────┴────────────┴────────────┴─────────────────┴─────────────────────┴─────────────────────┴─────────────┴──────────────┴────────────┴─────────┴─────────┴─────────────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/labs/hulab/mxwang/anaconda3/envs/S4M/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1023: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train encounters: 1956\n",
      "Test encounters : 652\n",
      "Saved: ./ppg_split_lists_stratified_hadm_labdemo.json\n",
      "Train: 1956\n",
      "Test : 652\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_tbl_width_chars(2000)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Paths / Config\n",
    "# ============================================================\n",
    "\n",
    "DEMO_PATH   = \"./mimic_patient_admission_demo_with_diag.csv\"\n",
    "\n",
    "# NEW unified lab+vital files\n",
    "HIST_PATH   = \"./mimic_lab_vital_waveform_history.csv\"\n",
    "TARGET_PATH = \"./mimic_lab_vital_waveform_overlap.csv\"\n",
    "\n",
    "HQ_JSON     = \"./mimic_high_quality_info_list.json\"\n",
    "OUT_SPLIT_JSON = \"./ppg_split_lists_stratified_hadm_labdemo.json\"\n",
    "\n",
    "lab_cols   = [\"Potassium\", \"Calcium\", \"Sodium\", \"Glucose\", \"Lactate\", \"Creatinine\"]\n",
    "vital_cols = [\"NBPs\", \"NBPd\", \"NBPm\"]\n",
    "nbp_cols   = vital_cols[:]  # three NBP signals\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Load CSVs\n",
    "# ============================================================\n",
    "\n",
    "demo   = pl.read_csv(DEMO_PATH,   infer_schema_length=20000)\n",
    "hist   = pl.read_csv(HIST_PATH,   infer_schema_length=20000)\n",
    "target = pl.read_csv(TARGET_PATH, infer_schema_length=20000)\n",
    "\n",
    "# vital_raw is simply the target (all vitals included in overlap)\n",
    "vital_raw = target\n",
    "\n",
    "print(\"Loaded CSVs:\")\n",
    "print(\" demo   :\", demo.shape)\n",
    "print(\" hist   :\", hist.shape)\n",
    "print(\" target :\", target.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Load HQ waveform encounters from mimic_high_quality_info_list.json\n",
    "# ============================================================\n",
    "\n",
    "with open(HQ_JSON, \"r\") as f:\n",
    "    ppg_meta = json.load(f)\n",
    "\n",
    "encounters = []\n",
    "for entry in ppg_meta:\n",
    "    fname = entry[0]\n",
    "    parts = fname.split(\"_\")\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    subj = parts[0]\n",
    "    hadm = parts[1]\n",
    "    encounters.append((subj, hadm))\n",
    "\n",
    "encounters = sorted(set(encounters))\n",
    "print(\"Unique waveform encounters:\", len(encounters))\n",
    "\n",
    "base_enc = pl.DataFrame({\n",
    "    \"SUBJECT_ID\": [p[0] for p in encounters],\n",
    "    \"HADM_ID\":    [p[1] for p in encounters],\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3B. Fix dtype mismatches for joining\n",
    "# ============================================================\n",
    "\n",
    "to_utf8 = [\"SUBJECT_ID\", \"HADM_ID\"]\n",
    "\n",
    "base_enc = base_enc.with_columns([pl.col(c).cast(pl.Utf8) for c in to_utf8])\n",
    "demo      = demo.with_columns([pl.col(c).cast(pl.Utf8) for c in to_utf8])\n",
    "hist      = hist.with_columns([pl.col(c).cast(pl.Utf8) for c in to_utf8])\n",
    "target    = target.with_columns([pl.col(c).cast(pl.Utf8) for c in to_utf8])\n",
    "vital_raw = vital_raw.with_columns([pl.col(c).cast(pl.Utf8) for c in to_utf8])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Join encounters\n",
    "# ============================================================\n",
    "\n",
    "demo_enc   = demo.join(base_enc,   on=to_utf8, how=\"inner\")\n",
    "hist_enc   = hist.join(base_enc,   on=to_utf8, how=\"inner\")\n",
    "target_enc = target.join(base_enc, on=to_utf8, how=\"inner\")\n",
    "\n",
    "print(\"Joined:\")\n",
    "print(\" demo_enc   :\", demo_enc.shape)\n",
    "print(\" hist_enc   :\", hist_enc.shape)\n",
    "print(\" target_enc :\", target_enc.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Demographics\n",
    "# ============================================================\n",
    "\n",
    "demo_stats = (\n",
    "    demo_enc\n",
    "    .group_by(to_utf8)\n",
    "    .agg([\n",
    "        pl.col(\"GENDER\").first().alias(\"GENDER\"),\n",
    "        pl.col(\"ETHNICITY\").first().alias(\"ETHNICITY\"),\n",
    "        pl.col(\"INSURANCE\").first().alias(\"INSURANCE\"),\n",
    "        pl.col(\"LANGUAGE\").first().alias(\"LANGUAGE\"),\n",
    "        pl.col(\"MARITAL_STATUS\").first().alias(\"MARITAL_STATUS\"),\n",
    "        pl.col(\"ICD9_CODE\").first().alias(\"ICD9_CODE\"),\n",
    "        pl.col(\"age_at_admit\").mean().alias(\"age_at_admit\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Lab history stats\n",
    "# ============================================================\n",
    "\n",
    "hist_stats = (\n",
    "    hist_enc\n",
    "    .select(to_utf8 + lab_cols)\n",
    "    .group_by(to_utf8)\n",
    "    .agg(\n",
    "        [pl.col(c).min().alias(f\"{c}_hist_min\") for c in lab_cols] +\n",
    "        [pl.col(c).max().alias(f\"{c}_hist_max\") for c in lab_cols]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add lab ranges\n",
    "for lab in lab_cols:\n",
    "    hist_stats = hist_stats.with_columns(\n",
    "        (pl.col(f\"{lab}_hist_max\") - pl.col(f\"{lab}_hist_min\")).alias(f\"{lab}_hist_range\")\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Lab target means (labs during overlap)\n",
    "# ============================================================\n",
    "\n",
    "target_stats = (\n",
    "    target_enc\n",
    "    .select(to_utf8 + lab_cols)\n",
    "    .group_by(to_utf8)\n",
    "    .agg([pl.col(c).mean().alias(f\"{c}_target_mean\") for c in lab_cols])\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Vital stats (NBP coverage)\n",
    "# ============================================================\n",
    "\n",
    "vital_enc = vital_raw.join(base_enc, on=to_utf8, how=\"inner\").with_columns(\n",
    "    pl.col(\"CHARTTIME\").str.to_datetime(strict=False)\n",
    ")\n",
    "\n",
    "vital_stats = (\n",
    "    vital_enc\n",
    "    .select(to_utf8 + vital_cols)\n",
    "    .group_by(to_utf8)\n",
    "    .agg([pl.col(c).count().alias(f\"{c}_count\") for c in vital_cols])\n",
    ")\n",
    "\n",
    "# total NBP count\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    sum(pl.col(f\"{c}_count\") for c in nbp_cols).alias(\"NBP_total_count\")\n",
    ")\n",
    "\n",
    "# coverage window\n",
    "dur_df = (\n",
    "    vital_enc\n",
    "    .group_by(to_utf8)\n",
    "    .agg([\n",
    "        pl.col(\"CHARTTIME\").min().alias(\"vital_start\"),\n",
    "        pl.col(\"CHARTTIME\").max().alias(\"vital_end\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "vital_stats = vital_stats.join(dur_df, on=to_utf8, how=\"left\")\n",
    "\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    ((pl.col(\"vital_end\") - pl.col(\"vital_start\")).dt.total_seconds().fill_null(0) / 3600.0)\n",
    "    .alias(\"vital_hours\")\n",
    ")\n",
    "\n",
    "vital_stats = vital_stats.with_columns(\n",
    "    (pl.col(\"NBP_total_count\") / (pl.col(\"vital_hours\") + 1e-6)).alias(\"NBP_per_hour\")\n",
    ")\n",
    "\n",
    "print(\" vital_stats:\", vital_stats.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. Merge all summary stats\n",
    "# ============================================================\n",
    "\n",
    "summary = (\n",
    "    base_enc\n",
    "    .join(demo_stats,   on=to_utf8, how=\"left\")\n",
    "    .join(hist_stats,   on=to_utf8, how=\"left\")\n",
    "    .join(target_stats, on=to_utf8, how=\"left\")\n",
    "    .join(vital_stats,  on=to_utf8, how=\"left\")\n",
    "    .fill_null(0)\n",
    ")\n",
    "\n",
    "# gender + age bins\n",
    "summary = summary.with_columns([\n",
    "    pl.when(pl.col(\"GENDER\") == \"M\").then(0).otherwise(1).alias(\"gender_bin\"),\n",
    "    pl.when(pl.col(\"age_at_admit\") < 30).then(1)\n",
    "     .when(pl.col(\"age_at_admit\") < 50).then(2)\n",
    "     .when(pl.col(\"age_at_admit\") < 70).then(3)\n",
    "     .when(pl.col(\"age_at_admit\") < 90).then(4)\n",
    "     .otherwise(5)\n",
    "     .alias(\"age_bin\"),\n",
    "])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. Vital coverage → bins\n",
    "# ============================================================\n",
    "\n",
    "nbp_vals = summary[\"NBP_per_hour\"].to_numpy()\n",
    "finite = np.isfinite(nbp_vals)\n",
    "\n",
    "if finite.sum() > 10:\n",
    "    q = np.nanquantile(nbp_vals[finite], [0.2, 0.4, 0.6, 0.8])\n",
    "    summary = summary.with_columns(\n",
    "        pl.when(pl.col(\"NBP_per_hour\") < q[0]).then(0)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[1]).then(1)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[2]).then(2)\n",
    "         .when(pl.col(\"NBP_per_hour\") < q[3]).then(3)\n",
    "         .otherwise(4)\n",
    "         .alias(\"NBP_bin\")\n",
    "    )\n",
    "else:\n",
    "    summary = summary.with_columns(pl.lit(2).alias(\"NBP_bin\"))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 11. Final strata key\n",
    "# ============================================================\n",
    "\n",
    "combo_cols = [\n",
    "    \"age_bin\",\"gender_bin\",\"ETHNICITY\",\"INSURANCE\",\n",
    "    \"LANGUAGE\",\"MARITAL_STATUS\",\"ICD9_CODE\",\"NBP_bin\",\n",
    "]\n",
    "\n",
    "def combine_fields(df, cols):\n",
    "    out = df[cols[0]].cast(pl.Utf8)\n",
    "    for c in cols[1:]:\n",
    "        out = out + \"_\" + df[c].cast(pl.Utf8)\n",
    "    return out\n",
    "\n",
    "summary = summary.with_columns(\n",
    "    combine_fields(summary, combo_cols).alias(\"strata\")\n",
    ")\n",
    "\n",
    "# ensure sklearn-friendly types\n",
    "categorical_cols = [\n",
    "    \"ETHNICITY\", \"INSURANCE\", \"LANGUAGE\",\n",
    "    \"MARITAL_STATUS\", \"ICD9_CODE\", \"strata\"\n",
    "]\n",
    "\n",
    "summary = summary.with_columns([\n",
    "    pl.col(c).cast(pl.Utf8).fill_null(\"UNK\") for c in categorical_cols\n",
    "])\n",
    "\n",
    "summary_pd = summary.to_pandas()\n",
    "summary_pd[\"strata\"] = summary_pd[\"strata\"].astype(str)\n",
    "\n",
    "print(\"Final summary shape:\", summary.shape)\n",
    "print(summary.head(5))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 12. StratifiedGroupKFold\n",
    "# ============================================================\n",
    "\n",
    "hadm_ids = summary_pd[\"HADM_ID\"].to_numpy()\n",
    "strata   = summary_pd[\"strata\"].to_numpy()\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "train_idx, test_idx = next(sgkf.split(hadm_ids, strata, groups=hadm_ids))\n",
    "\n",
    "train_hadm = set(hadm_ids[train_idx])\n",
    "test_hadm  = set(hadm_ids[test_idx])\n",
    "\n",
    "print(\"Train encounters:\", len(train_hadm))\n",
    "print(\"Test encounters :\", len(test_hadm))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 13. Assign waveform files to splits\n",
    "# ============================================================\n",
    "\n",
    "split_dict = {\n",
    "    \"train_control_list\": [],\n",
    "    \"test_control_list\": [],\n",
    "}\n",
    "\n",
    "for entry in ppg_meta:\n",
    "    hadm = entry[1]  # already string or int-like\n",
    "    if str(hadm) in train_hadm or int(hadm) in train_hadm:\n",
    "        split_dict[\"train_control_list\"].append(entry)\n",
    "    elif str(hadm) in test_hadm or int(hadm) in test_hadm:\n",
    "        split_dict[\"test_control_list\"].append(entry)\n",
    "\n",
    "with open(OUT_SPLIT_JSON, \"w\") as f:\n",
    "    json.dump(split_dict, f, indent=4)\n",
    "\n",
    "print(\"Saved:\", OUT_SPLIT_JSON)\n",
    "print(\"Train:\", len(split_dict[\"train_control_list\"]))\n",
    "print(\"Test :\", len(split_dict[\"test_control_list\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "S4M",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
